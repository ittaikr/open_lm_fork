job_details:
  name: openhermes_dog_6layers_decay_to_init
  output_dir: results
  num_gpus: 1
  num_nodes: 1
parameters:
    save_checkpoint: [false]
    LIST_MODEL_WARMUP_TOKENS:

      - model: ['layers=6_hidden-dim=224']
        max_tokens: [2069299200] 
      
    warmup: [0] # 6115
    train_num_samples: [10000000]
    val_num_samples: [2000]
    # ~130B as in kaplan (note that we set 1000 epochs for avoiding repeating too much data in resuming),
    # but we will never reach that - we will stop after 6 or 12 hours.
    dataset_resampled: [false]
    precision: ['bfloat16']
    
    LIST_OF_BATCHES:
      - batch_size: [4,1] # global batch size is 16*4=64
        accum_freq: [1]
      - batch_size: [16] # global batch size is 16*4=64
        accum_freq: [8]

    grad_checkpointing: [true]
    log_every_n_steps: [20]
    csv_log: [true]
    grad_clip_norm: [null]
    #beta1: [0.9, 0.0]
    #beta2: [0.95]
    eps: [1.0e-08]
    report_to: [""] # 'wandb'
    resume: ['latest']
    data_key: ['txt']
    #lr_cooldown_end: [3.0e-05] # in the rpj config it was like that, so I keep it
    qk_norm: [true]
    z_loss_coefficient: [0.0001]
    lr_scheduler: ['const']
    wd: [0.001, 0.00033, 0.0001, 0.000033, 0.00001, 0.0000033, 0]
    lr: [1.0]
    epochs: [50]
    wandb_project_name: ['openhermes_test']
    delete_previous_checkpoint: [true]
    workers: [4]
    train_data: ["/home/ycarmon/data/wds/openhermes/shard-{0000000..0000023}.tar"]
    val_data: ["/home/ycarmon/data/wds/openhermes/shard-{0000024..0000025}.tar"]
    averagers: ['poly_32_1']
    optim_alg: ['dog','accele_dog']
    dog_granularity: ['all', 'param']
    dog_init_dist_rel: [1.0e-06]
    dog_decay_to_init: [true]
    decouple_decay: [true]
    decay_factor: [1.0]
    # if there are 600B tokens in RW, and all of them are uniformly distributed in the shards, then each shard has 600B/42512 = 14.1M tokens
    # so, keeping 1000 shards for validation, we have 1000*14.1M = 14.1B tokens for validation