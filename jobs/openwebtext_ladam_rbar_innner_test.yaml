job_details:
  name: openwebtext_adog_inner_layers10_test #
  output_dir: results
  num_gpus: 1
  num_nodes: 1
parameters:
    save_checkpoint: [true]
    LIST_MODEL_WARMUP_TOKENS:

      # - model: ['layers=4_hidden-dim=128']
      #   max_tokens: [2069299200]
      # - model: ['layers=6_hidden-dim=224']
      #   max_tokens: [2069299200]
      # - model: ['layers=8_hidden-dim=288']
      #   max_tokens: [2069299200]
      - model: ['layers=10_hidden-dim=384']
        max_tokens: [2069299200]
        #pretrained: ["results/24-10-08-openwebtext_adog_save_layers10_1e-5/000_24-10-08-openwebtext_adog_save_layers10_1e-5+/checkpoints/epoch_12.pt"]
      
    train_num_samples: [10000000]
    val_num_samples: [2000]
    # ~130B as in kaplan (note that we set 1000 epochs for avoiding repeating too much data in resuming),
    # but we will never reach that - we will stop after 6 or 12 hours.
    dataset_resampled: [false]
    precision: ['float16']
    LIST_OF_BATCHES:
      # - batch_size: [1]
      #   accum_freq: [1]
      #   val_freq: [1]
      # - batch_size: [4] # global batch size is 16*4=64
      #   accum_freq: [1]
      #   val_freq: [1]
      # - batch_size: [16]
      #   accum_freq: [4]
      #   val_freq: [1]
      - batch_size: [64]
        accum_freq: [16]
        val_freq: [4]
      # - batch_size: [2]
      #   accum_freq: [1]
      #   val_freq: [1]
      # - batch_size: [8]
      #   accum_freq: [2]
      #   val_freq: [1]
      # - batch_size: [32]
      #   accum_freq: [8]
      #   val_freq: [2]
      # - batch_size: [128]
      #   accum_freq: [32]
      #   val_freq: [8]
    grad_checkpointing: [true]
    log_every_n_steps: [20]
    csv_log: [true]
    grad_clip_norm: [1] # [null]
    fake_grad_clip_norm: [1]
    eps: [1.0e-08]
    report_to: [''] # 'wandb'
    resume: ['latest']
    data_key: ['json.gz']
    lr_cooldown_end: [3.0e-05] # in the rpj config it was like that, so I keep it
    qk_norm: [true]
    z_loss_coefficient: [0.0001]
    epochs: [2] # [100]
    # save_frequency: [1]
    # keep_freq: [5]
    #reset_opt: ["10"]
    wandb_project_name: [null]
    delete_previous_checkpoint: [true]
    workers: [4]
    dataset_manifest: ["data/openwebtext2_tokenized/manifest_train.jsonl"] # ["data/openwebtext2_tokenized/manifest_train.jsonl"]
    val_data: ["data/openwebtext2_tokenized/00000026.tar"]
    averagers: ['poly_8_1']
    LIST_OF_OPTIM:
      - optim_alg: ['accele_dog'] # ['dog'] # 
        ladamw_g_op: ['avg']
        warmup_tokens: eval("[ 0 ]")
        lr_scheduler: ['const']
        moving_avg_init: [null] # eval("[f'ema2_{1-0.1**k}' for k in range(1,5)] + ['ema2_1.0']") # ['ema_0.99'] # ['poly_8_1']
        decoupled_wd: [0.]
        lr: [1.0]
        beta1: [1.]
        beta2: [1.]
        dog_init_dist_rel: eval("[10**(k/2) for k in range(-6,-1)]") # [1.0e-02] # eval("[10**(k) for k in range(-5,0)]") # [1.0e-03] # eval("[10**(k/2) for k in range(-12,6)]") # eval("[10**(k/2) for k in range(-12,0)]")
        bias_init_dist_rel: [null] # eval("[10**(k/2) for k in range(-12,0)]") # [1.0e-03]
        embeddings_init_dist_rel: [2.] # [1.0e-03] # eval("[10**(k/2) for k in range(0,6)]")
        output_init_dist_rel: [1.0e-03] # eval("[10**(k/2) for k in range(-12,0)]")
        dog_decay_to_init: [false]
    LIST_OF_RBARS:
      # - normalize_rbar: [true]
      #   granularity_per_param: [true]
      - normalize_rbar: [false]
        granularity_per_param: [false]
    max_rbar: [false]
    decouple_decay: [true]
    decay_factor: [1.0]
    dog_granularity: ['param']
    granularity_norm: [false]
    granularity_per_token: [true] # [false] # 
    granularity_attentions: [false] # [true]
    granularity_feed_forward: [false] # [true]
    split_attn_proj: [true]
    granularity_per_head: [true]
    constant_weight_norm: ['input,output'] # ['all'] # [null] #
    always_project: [true]
    save_grads_norms: [true]
    save_layers_norms: [true]
    # if there are 600B tokens in RW, and all of them are uniformly distributed in the shards, then each shard has 600B/42512 = 14.1M tokens
    # so, keeping 1000 shards for validation, we have 1000*14.1M = 14.1B tokens for validation